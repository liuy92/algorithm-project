#简单讲解文本挖掘的相关处理过程

数据预处理：
1. 去掉html的一些tag标记
2. 禁用词去除，词根还原
3. 中文分词、词性标注、短语识别
4. 词频统计
5. 数据清洗

衡量词语的权重
1. 布尔权重(该词是否存在)
2. 绝对词频(TF = 该词在文档-样本中出现的次数)
3. 倒排文档频度(IDF = log(文档数/出现该词的文档-样本数))
4. TF-IDF (绝对词频 * 倒排文档频度)
5. TF-IWF ()

文本挖掘操作
1. 特征提取
   1.1 按照包含词条的文档-样本数进行特征提取，筛除出现频率较小的词语
   1.2 熵 —— 确定分类类别，加入不同的词条后体统最大的熵值
   1.3 信息增益 分别计算加入词条前后的系统的熵的变化值
   备注：太频繁的词没有区分度，太稀有的词表达性不强，对全局分类效果作用不大
         据baeza-Yates and Ribeiro-Neto 研究表明，出现在80%以上的文档中的词对分类无意义
         据Yang and Pedersen 实验表明：分类效果在缩减维度为原来的10%是有可能损失很小的
         需要注意的是在专属文档高频出现的低频次 —— 即出现在少数文档中，但是在出现的文档中频繁出现
2. 特征重构 —— 潜在语义的分析
   对矩阵进行分解 SVD，可得 AV=UK (用K暂代amiga)
   即 A=UKV'  其中 U'U = I， V'V = I 
   则可用U代表文档和潜在语义之间的关系矩阵，V作为词和潜在语义之间的关系矩阵


一般处理步骤：
1. 对不同文档的文本进行分词
2. 分别计算每个词在每个文档中出现的次数ti，以及其出现的文档数ni
3. 计算每个词的相关权重，并进行排序，并依照先关规则，对词语进行特征提取
4. 按照特征提取后的词，构建相关主题模型进行潜在语义分析 LSA