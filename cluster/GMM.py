

#高斯混合模型 —— GMM (Gaussian Mixture Model)
"""
统计学模型
1. 概率模型(软分类)
   基于贝叶斯原理，所有事件的发生受到另一个事件的影响。即事件的发生对应的是一系列发生可能性，选择发生概率最大的类作为判决对象
2. 非概率模型(硬分类)
   依据输入数据判别唯一所处的类别
"""
"""
GMM
实质：依据高斯模型训练出每个类别对应的概率分布，混合多个高斯模型得到最终估计模型
公式：
    混合高斯模型：p(x) = sum(pi_k * p(x | k))   [k <= K]
                  说明 -- K 为模型个数(类别数)，pi_k 为第k个高斯模型的权重，p(x | k)为第k个高斯模型的概率分布
                  参数 -- pi_k, mu_k, sigma_k
                  求解 -- 对数最大似然 + EM 算法
                  问题 -- 数据量N很大时，由于连乘的结果非常小，容易造成浮点下溢
                  解决 -- 对数化
    对数最大似然：MLE  -- max(sum(log(p(x_i)))) = max(sum(log(sum(pi_k * p(x_i | mu_k, sigma_k)))))
    EM算法：1. 初始化所有的参数，按照样本迭代出每个高斯模型的权值
               对样本x_i 来说，归属第k个类别(高斯模型)的概率：
                    w_i_k = pi_k * N_k / sum(pi_j * N_j)
            2. 基于估计的权值，来计算高斯模型的参数(MLE)
                    mu_k = sum(w_i_k * x_i) / N 
                    sigma_k = sum(w_i_k * (x_i - mu_k) * t(x_i - Mu_k)) / N 
                    N_k = sum(w_i_k)
            3. 重复迭代步骤1、2，直到收敛~
            问题 -- 容易局部最优
算法：
    1. 初始化类别和每个类别的中心点
    2. 计算所有
"""