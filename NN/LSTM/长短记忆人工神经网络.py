#-*-coding:utf-8-*-

##z主要学习长度那记忆单元的相关基础(LSTM)
"""
LSTM:
实质：循环神经网络
用处：识别文本、基因、笔记、语音等序列数据；识别传感器、股票等生成的数值型时间序列数据
"""
"""
循环网络与前馈网络：
* 前馈神经网络：神经元的信号逐层单向传递，网络无反馈，可用有向无环图表示
  也称为多层感知器，由多层的logistic回归模型组成。没有时间顺序的概念
  误差反馈：将最终误差反向按比例分配给梅阁全种，通过计算权重与误差的偏导数，利用梯度下降对权重进行调整
* 循环神经网络：神经元的信号单向循环传递，输入包含当前输入样例以及之前感知信息，有向有环图
"""
"""
循环神经网络：
实质：增加时间序列原理，将过去的结果作为一个特征作用于现在的结果
公式： h_t = φ(W * X_t + U * h_t-1)
      其中：W为神经元的权重矩阵；X_t为当前时间的输入；h_t-1为前一事件的隐藏状态；U为隐藏状态矩阵(过度矩阵)，φ为和函数的转化
      W决定当前输入和过去隐藏状态的重要性分配。其产生的误差会反向传播返回来调整权重W
      φ 往往采用逻辑S型函数(Sigmoid函数)或双曲正切函数。将过大或者过小的值转化为一个标准数据，并产生梯度
误差反馈：BPTT(沿时间反向反馈)[表示没太看懂]

问题：梯度消失
描述：梯度表示权重随误差的变化而发生的改变，循环网络需要确定最终输出和不同时间的结果的联系。
      过往经历的重要程度难以评估，神经网络中每层之间存在大量乘积运算，更加难以分配重要度
      由于深度神经网络的层和时间步通过乘法连接，由复合利率可以发现，导数会出现膨胀或者缩减
      梯度膨胀：权重的梯度增加至饱和，导致重要性过高
      梯度缩减：权重的梯度缩减至消失，网络无法学习
解决：single sigmoids的缩胀程度最为明显；其次double sigmoids；然后triple sigmoids；quadruple sigmoids效果最好
"""
"""
长短记忆单元(LSTM)
优势：很好的解决了梯度消失的问题
机制：http://www.jianshu.com/p/9dc9f41f0b29
"""